{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6164bc61-0d03-47b1-8df6-72077ac3367a",
   "metadata": {},
   "source": [
    "# FXの将来のリターン動向の予測モデル\n",
    "\n",
    "目的：過去18時間の **\"usdjpy\",\"cadjpy\"** を使って、将来**5分後**のリターン動向を予測する   \n",
    "モデル：GCN   \n",
    "開発環境: python 3.11.5/ JupyterLab 3.6.3/Jupyter Notebook Version: 6.5.4/System: Linux #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88f29210-8a01-473c-8d80-36bb27d4eb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cda929-1ba4-4cc5-b685-15396c52104d",
   "metadata": {},
   "source": [
    "Pandas version: 2.0.3   \n",
    "Numpy version: 1.24.3   \n",
    "sklearn: 1.3.0   \n",
    "sklearn: 0.6.3   \n",
    "torch: 2.1.1   \n",
    "torch_geometric: 2.4.0   \n",
    "tqdm: 4.65.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bf7e703c-3832-499a-a82a-1670710f908b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Times</th>\n",
       "      <th>Final Price</th>\n",
       "      <th>Final Price Normalized</th>\n",
       "      <th>5m_Return</th>\n",
       "      <th>Label</th>\n",
       "      <th>Currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-17 18:10:00</td>\n",
       "      <td>109.133</td>\n",
       "      <td>0.156310</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17 18:11:00</td>\n",
       "      <td>109.114</td>\n",
       "      <td>0.155936</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-17 18:12:00</td>\n",
       "      <td>109.117</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-17 18:13:00</td>\n",
       "      <td>109.111</td>\n",
       "      <td>0.155877</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-17 18:14:00</td>\n",
       "      <td>109.110</td>\n",
       "      <td>0.155857</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Times  Final Price  Final Price Normalized  5m_Return  Label  \\\n",
       "0  2019-01-17 18:10:00      109.133                0.156310  -0.000192      0   \n",
       "1  2019-01-17 18:11:00      109.114                0.155936  -0.000421      0   \n",
       "2  2019-01-17 18:12:00      109.117                0.155995  -0.000421      0   \n",
       "3  2019-01-17 18:13:00      109.111                0.155877  -0.000577      0   \n",
       "4  2019-01-17 18:14:00      109.110                0.155857  -0.000394      0   \n",
       "\n",
       "   Currency  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Times</th>\n",
       "      <th>Final Price</th>\n",
       "      <th>Final Price Normalized</th>\n",
       "      <th>5m_Return</th>\n",
       "      <th>Label</th>\n",
       "      <th>Currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-17 18:10:00</td>\n",
       "      <td>82.159</td>\n",
       "      <td>0.222335</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17 18:11:00</td>\n",
       "      <td>82.146</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-17 18:12:00</td>\n",
       "      <td>82.146</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-17 18:13:00</td>\n",
       "      <td>82.144</td>\n",
       "      <td>0.221933</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-17 18:14:00</td>\n",
       "      <td>82.144</td>\n",
       "      <td>0.221933</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Times  Final Price  Final Price Normalized  5m_Return  Label  \\\n",
       "0  2019-01-17 18:10:00       82.159                0.222335  -0.000280      0   \n",
       "1  2019-01-17 18:11:00       82.146                0.221987  -0.000487      0   \n",
       "2  2019-01-17 18:12:00       82.146                0.221987  -0.000523      0   \n",
       "3  2019-01-17 18:13:00       82.144                0.221933  -0.000633      0   \n",
       "4  2019-01-17 18:14:00       82.144                0.221933  -0.000365      0   \n",
       "\n",
       "   Currency  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "'''\n",
    "9 currency pairs from 2018-01-01 18:10:00 to 2023-11-17 16:10:00\n",
    "'''\n",
    "\n",
    "def load_dataset(filename, start_date, end_date):\n",
    "    \n",
    "    df = pd.read_csv(f'5minutes_processed_data/{filename}.csv',sep = ',')\n",
    "    df.set_index('Times', inplace=True)\n",
    "    \n",
    "    df = df.loc[start_date:end_date]\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_date = '2019-01-17 18:10:00'\n",
    "end_date = '2023-11-17 16:10:00'\n",
    "\n",
    "variable_names = [\"usdjpy\",\"cadjpy\"]\n",
    "for name in variable_names:\n",
    "    exec(f\"{name} = load_dataset(str.upper('{name}'), start_date, end_date)\")\n",
    "    exec(f\"display({name}.head())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "594c34e0-a2ab-4c17-85f7-4af0b2283022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Times</th>\n",
       "      <th>Final Price</th>\n",
       "      <th>Final Price Normalized</th>\n",
       "      <th>5m_Return</th>\n",
       "      <th>Label</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Times</th>\n",
       "      <th>Final Price</th>\n",
       "      <th>Final Price Normalized</th>\n",
       "      <th>5m_Return</th>\n",
       "      <th>Label</th>\n",
       "      <th>Currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-17 18:10:00</td>\n",
       "      <td>109.133</td>\n",
       "      <td>0.156310</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-17 18:10:00</td>\n",
       "      <td>82.159</td>\n",
       "      <td>0.222335</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17 18:11:00</td>\n",
       "      <td>109.114</td>\n",
       "      <td>0.155936</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-17 18:11:00</td>\n",
       "      <td>82.146</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-17 18:12:00</td>\n",
       "      <td>109.117</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-17 18:12:00</td>\n",
       "      <td>82.146</td>\n",
       "      <td>0.221987</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-17 18:13:00</td>\n",
       "      <td>109.111</td>\n",
       "      <td>0.155877</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-17 18:13:00</td>\n",
       "      <td>82.144</td>\n",
       "      <td>0.221933</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-17 18:14:00</td>\n",
       "      <td>109.110</td>\n",
       "      <td>0.155857</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-17 18:14:00</td>\n",
       "      <td>82.144</td>\n",
       "      <td>0.221933</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Times  Final Price  Final Price Normalized  5m_Return  Label  \\\n",
       "0  2019-01-17 18:10:00      109.133                0.156310  -0.000192      0   \n",
       "1  2019-01-17 18:11:00      109.114                0.155936  -0.000421      0   \n",
       "2  2019-01-17 18:12:00      109.117                0.155995  -0.000421      0   \n",
       "3  2019-01-17 18:13:00      109.111                0.155877  -0.000577      0   \n",
       "4  2019-01-17 18:14:00      109.110                0.155857  -0.000394      0   \n",
       "\n",
       "   Currency                Times  Final Price  Final Price Normalized  \\\n",
       "0         0  2019-01-17 18:10:00       82.159                0.222335   \n",
       "1         0  2019-01-17 18:11:00       82.146                0.221987   \n",
       "2         0  2019-01-17 18:12:00       82.146                0.221987   \n",
       "3         0  2019-01-17 18:13:00       82.144                0.221933   \n",
       "4         0  2019-01-17 18:14:00       82.144                0.221933   \n",
       "\n",
       "   5m_Return  Label  Currency  \n",
       "0  -0.000280      0         1  \n",
       "1  -0.000487      0         1  \n",
       "2  -0.000523      0         1  \n",
       "3  -0.000633      0         1  \n",
       "4  -0.000365      0         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine all the datasets\n",
    "combined_df = pd.concat([usdjpy, cadjpy],axis=1)\n",
    "display(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91bbceed-0004-4271-b714-b3697ed08671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute technical indicator\n",
    "'''\n",
    "Calculates each technical indicator using the entire hour for each node.\n",
    "'''\n",
    "\n",
    "# Calculate Bollinger Bands for the entire hour\n",
    "def calculate_bollinger_bands(data):\n",
    "    window = len(data)\n",
    "    rolling_mean = data.rolling(window=window).mean()\n",
    "    upper_band = rolling_mean + 2 * data.rolling(window=window).std()\n",
    "    lower_band = rolling_mean - 2 * data.rolling(window=window).std()\n",
    "    \n",
    "    return upper_band.iloc[-1], lower_band.iloc[-1], rolling_mean.iloc[-1]\n",
    "\n",
    "# Calculate RSI for the entire hour\n",
    "def calculate_rsi(data):\n",
    "    diff = data.diff(1).dropna()\n",
    "    gain = diff.where(diff > 0, 0)\n",
    "    loss = -diff.where(diff < 0, 0)\n",
    "\n",
    "    avg_gain = gain.mean()\n",
    "    avg_loss = loss.mean()\n",
    "\n",
    "    if avg_loss == 0:\n",
    "        rs = np.inf  # Set to infinity to avoid division by zero\n",
    "    else:\n",
    "        rs = avg_gain / avg_loss\n",
    "\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Calculate RCI for the entire hour\n",
    "def calculate_rci(data):\n",
    "    rci = data.pct_change().sum()\n",
    "    return rci\n",
    "\n",
    "# Calculate Momentum for the entire hour\n",
    "def calculate_momentum(data, n=59):\n",
    "    return data.diff(n).iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fde9457e-1e9a-48a7-b8a8-840836ede345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a single graph (snapshot)\n",
    "\n",
    "'''\n",
    "In each graph we use data within 18 hours.\n",
    "Each hour each currency pair forms a node, so each snapshot contains 18*9　nodes.\n",
    "Weighted edges are generated from computing DTW of two nodes (two 1 hour series) using \"Final Price Normalized\"\n",
    "Node features consist of \"Final Price Normalized\", \"1h_Return\", \"Currency\"　and technical indicator\n",
    "'''\n",
    "\n",
    "def create_graph(snapshot, node_window=60, node_stride=60):\n",
    "    \n",
    "    features = snapshot[[ 'Final Price Normalized','1h_Return', 'Currency', 'Final Price']].values\n",
    "    \n",
    "    series = []\n",
    "    currencies = []\n",
    "    prices = []\n",
    "    h_returns = []\n",
    "    Final_prices = []\n",
    "    volatilities = []\n",
    "    \n",
    "    upper_bands = []\n",
    "    lower_bands = []\n",
    "    rolling_means = []\n",
    "    rsis = []\n",
    "    rcis = []\n",
    "    momenta = []  \n",
    "    \n",
    "     \n",
    "    for i in range(0, len(snapshot) - node_window + 1, node_stride):\n",
    "        for j in range (2): \n",
    "            serie = features[i:i+node_window,j]\n",
    "            series.append(serie) \n",
    "            \n",
    "\n",
    "            currency = features[i+node_window-1,j+4]\n",
    "            currencies.append(currency)\n",
    "\n",
    "            price = features[i+node_window-1, j]\n",
    "            prices.append(price)\n",
    "            \n",
    "            Final_price = features[i:i+node_window, j+6]\n",
    "            Final_prices.append(Final_price)\n",
    "\n",
    "            h_return = features[i+node_window-1, j+2]\n",
    "            h_returns.append(h_return)\n",
    "            \n",
    "            # Create a DataFrame with the stock price data\n",
    "            df = pd.DataFrame({'Close': Final_price})\n",
    "\n",
    "            # Calculate Bollinger Bands\n",
    "            upper_band, lower_band, mean = calculate_bollinger_bands(df['Close'])\n",
    "            upper_bands.append(upper_band)\n",
    "            lower_bands.append(lower_band)\n",
    "            rolling_means.append(mean)\n",
    "\n",
    "            # Calculate RSI for the entire hour using the function\n",
    "            rsi = calculate_rsi(df['Close'])\n",
    "            rsis.append(rsi)\n",
    "\n",
    "            # Calculate RCI for the entire hour using the function\n",
    "            rci = calculate_rci(df['Close'])\n",
    "            rcis.append(rci)\n",
    "            \n",
    "            # Calculate Momentum for the entire hour using the function\n",
    "            momentum = calculate_momentum(df['Close'])\n",
    "            momenta.append(momentum)\n",
    "            \n",
    "            all_return = features[i:i+node_window, j+2]\n",
    "            \n",
    "            # Create a DataFrame with the returns\n",
    "            df_r = pd.DataFrame({'Return': all_return})\n",
    "            # Compute volatility (standard deviation of returns)\n",
    "            volatility = df_r['Return'].std()\n",
    "            volatilities.append(volatility)\n",
    "            \n",
    "    # Edge generation \n",
    "    adjacency_matrix = np.zeros((len(series), len(series)))\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        for j in range(i+1, len(series)):\n",
    "            dtw_distance = dtw(series[i], series[j])\n",
    "\n",
    "            # Update the maximum observed DTW distance dynamically\n",
    "            max_dtw_distance = max(max_dtw_distance, dtw_distance) if 'max_dtw_distance' in locals() else dtw_distance\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        for j in range(i+1, len(series)):\n",
    "            dtw_distance = dtw(series[i], series[j])\n",
    "            similarity = 1 - (dtw_distance / max_dtw_distance)\n",
    "\n",
    "            \n",
    "            adjacency_matrix[i, j] = similarity\n",
    "            adjacency_matrix[j, i] = similarity\n",
    "            \n",
    "    np.fill_diagonal(adjacency_matrix, 1)\n",
    "    feature_matrix = np.transpose(np.vstack((prices, h_returns, currencies, upper_bands, lower_bands, rolling_means, rsis, rcis, momenta, volatilities)))\n",
    "\n",
    "    return adjacency_matrix, feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bb22bac-01f8-463e-bc26-aa33a0638aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create graphs\n",
    "'''\n",
    "Build a graph using 18 hours of data, then move forward by 1 hour and build the next graph using the next 18 hours of data,\n",
    "store them in a list graphs.\n",
    "Time window of each graph is 18*60, stride is 60.\n",
    "\n",
    "The label of each graph is the 'Lable' of the next hour's data, (\"Label\" is 1 if \"1h_return\" is larger than 1, 0 otherwise)\n",
    "Because we want to use each graph(18 hours data) to predict the next hour data.\n",
    "The label of each graph is a sequence of 9 binary values, each represents a currency.\n",
    "'''\n",
    "\n",
    "def create_graphs(data, time_window=18*60, stride=60):\n",
    "    adjacency_matrice = []\n",
    "    feature_matrice = []\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get label data\n",
    "    Label = data['Label'].values\n",
    "    \n",
    "    # Iterate through data with a sliding window\n",
    "    for i in tqdm(range(0, len(data) - 2*time_window + 1, stride)):\n",
    "    # Get a snapshot of the time series data\n",
    "        snapshot = data.iloc[i:i + time_window]\n",
    "        \n",
    "        # Generate adjacency matrix and feature matrix from the snapshot\n",
    "        adjacency_matrix, feature_matrix = create_graph(snapshot)\n",
    "        \n",
    "        # Get the label\n",
    "        label = labels_data[i + time_window + 6]\n",
    "        \n",
    "        # Create edge index indicating positions of non-zero elements\n",
    "        edge_index = torch.tensor([[i, j] for i in range(adjacency_matrix.shape[0]) for j in range(adjacency_matrix.shape[1]) if adjacency_matrix[i, j] != 0], dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Convert to PyTorch Tensors\n",
    "        x_tensor = torch.FloatTensor(feature_matrix)\n",
    "        y_tensor = torch.LongTensor(label)       \n",
    "\n",
    "        # Create a PyTorch Geometric Data object\n",
    "        graph = Data(x=x_tensor, edge_index=edge_index)\n",
    "        \n",
    "        # Append to the lists\n",
    "        graphs.append(graph)\n",
    "        labels.append(y_tensor)\n",
    "\n",
    "    return graphs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a49b0167-6cd3-4011-be81-76299f043895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GCN\n",
    "'''\n",
    "The task is graph classification.\n",
    "Output is the label of each graph which is a sequence of 9 binary values, each represents a currency.\n",
    "'''\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        # Readout layer, take feature-wise average values of all node embeddings and use it as a graph feature of an input graph (readout)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Transform a graph feature by a linear transformation\n",
    "        x= self.linear1(x)\n",
    "        \n",
    "        #Employ a sigmoid function as an activation function of the output layer\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1b5fdef-4e28-4350-b688-10613e038365",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bec7c2b921348a3a80695c726ae186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate all snapshots. \n",
    "graphs, labels = create_graphs(combined_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65990f1a-fe0b-4653-842d-be900b4e1da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "split_ratio = 0.85\n",
    "split_idx = int(split_ratio * len(graphs))\n",
    "\n",
    "train_data = graphs[:split_idx]\n",
    "train_labels = labels[:split_idx]\n",
    "test_data = graphs[split_idx:]\n",
    "test_labels = labels[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "830585de-3857-49ea-a308-9bbefc186044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set batch size for training\n",
    "batch_size = 48\n",
    "\n",
    "# Learning rate for optimization\n",
    "lr = 0.001\n",
    "\n",
    "# Create DataLoader for training and test data with specified batch size and shuffle\n",
    "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(test_data, test_labels)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNModel(input_dim=10, hidden_dim=112, output_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "157a78c5-a783-4a1e-9f27-5774f3eef76b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:50 \t loss: 0.507840\n",
      "Epoch:100 \t loss: 0.532939\n",
      "Epoch:150 \t loss: 0.599288\n",
      "Epoch:200 \t loss: 0.547090\n",
      "Epoch:250 \t loss: 0.550927\n",
      "Epoch:300 \t loss: 0.633916\n",
      "Epoch:350 \t loss: 0.583702\n",
      "Epoch:400 \t loss: 0.392616\n",
      "Epoch:450 \t loss: 0.581334\n",
      "Epoch:500 \t loss: 0.687798\n",
      "Epoch:550 \t loss: 0.549113\n",
      "Epoch:600 \t loss: 0.438626\n"
     ]
    }
   ],
   "source": [
    "# Training loop \n",
    "num_epochs = 600\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        graphs, labels = batch\n",
    "        graphs = graphs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(graphs)\n",
    "        output = output.float()\n",
    "        \n",
    "        # print(output.shape)\n",
    "        # print(labels.shape)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acd787b8-9723-4adc-b0e3-4aa231fbd95f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6340\n",
      "Precision: 0.6661\n",
      "Recall: 0.6361\n",
      "AUC: 0.7324\n",
      "AUPR: 0.7518\n",
      "F1 Score: 0.6174\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation during evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "        graphs, labels = batch\n",
    "        graphs = graphs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(graphs)\n",
    "        output = output.float()\n",
    "\n",
    "        # Convert probabilities and labels to numpy arrays for scikit-learn metrics\n",
    "        output_np = output.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "\n",
    "        all_predictions.append(output_np)\n",
    "        all_labels.append(labels_np)\n",
    "\n",
    "# Concatenate predictions and labels across batches\n",
    "all_predictions = np.concatenate(all_predictions).flatten()\n",
    "all_labels = np.concatenate(all_labels).flatten()\n",
    "\n",
    "# Binary classification thresholding (you can adjust the threshold if needed)\n",
    "threshold = 0.58\n",
    "binary_predictions = (all_predictions > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(all_labels, binary_predictions, average='macro')\n",
    "recall = recall_score(all_labels, binary_predictions, average='macro')\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions, average='macro')\n",
    "aupr = average_precision_score(all_labels, all_predictions)\n",
    "accuracy = accuracy_score(all_labels, binary_predictions)\n",
    "f1 = f1_score(all_labels, binary_predictions, average='macro')\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'AUC: {roc_auc:.4f}')\n",
    "print(f'AUPR: {aupr:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33882890-1e43-4a40-ad3a-24d848e2af24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dec59d-10a0-4454-9254-8ae1a0821667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
